{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATION PRELIMINAIRE DU CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARCOURS DES DOCUMENTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration des documents: répartition selon les journaux, nombres d'exemplaires, dates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vérification du nombre de document dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/caf/\"\n",
    "files = sorted(os.listdir(path))\n",
    "len(files)\n",
    "#Nous avons un fichier supplémentaire \"README.txt\" ce qui justifie le nombre total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manipulation du nom des fichiers pour réléver les informations de publication ( date et source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = [str(year) for year in range(1835, 1969)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_decade = defaultdict(int)\n",
    "count_month = defaultdict(int)\n",
    "count_newspapers = defaultdict(int)\n",
    "covered_years = set()\n",
    "\n",
    "for f in files:\n",
    "    if \"_\" in f and f.endswith(\"txt\"):\n",
    "        elems = f.split(\"_\")\n",
    "        \n",
    "        newspaper = elems[1]\n",
    "        \n",
    "        year = elems[2].split(\"-\")[0]\n",
    "        covered_years.add(year)\n",
    "        decade = year[:3] + \"0s\"\n",
    "        \n",
    "        month = int(elems[2].split(\"-\")[1])\n",
    "        \n",
    "        count_decade[decade] += 1\n",
    "        count_newspapers[newspaper] += 1\n",
    "        count_month[month] += 1\n",
    "        \n",
    "    else:\n",
    "        print(f\"Anomalous file: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classement des articles selon les jouneaux qui les ont publiés \n",
    "\n",
    "print(f\"Il y a {count_newspapers['JB421']} exemplaires du journal L'Avenir du Luxembourg, {count_newspapers['JB427']} exemplaires de La Libre Belgique, {count_newspapers['JB555']} exemplaires de L'Indépendance belge, {count_newspapers['JB555A']} exemplaires de L'Indépendance belge (édité en Angleterre), {count_newspapers['JB567']} exemplaires du Journal de Bruxelles, {count_newspapers['JB572']} exemplaires du Journal de Charleroi, {count_newspapers['JB638']} exemplaires de La Meuse, {count_newspapers['JB685']} exemplaires du journal Le petit Bleu, {count_newspapers['JB2729']} exemplaires du journal Le Vingtième Siècle, {count_newspapers['JB773']} exemplaire de Vers l'Avenir, {count_newspapers['JB837']} exemplaires du journal Le Peuple, {count_newspapers['JB838']} exemplaire du journal Le Soir et {count_newspapers['JB1051']} exemplaires du journal Le Drapeau Rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des années sans aucun article sur le sujet \n",
    "\n",
    "missing_years = [y for y in all_years if y not in covered_years]\n",
    "print(f\"Années manquantes: {', '.join(missing_years)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la distribution du vocabulaire du corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création de notre liste de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"autre\", \"jusqu\", \"déjà\", \"heures\", \"très\", \"puis\", \n",
    "       \"vers\", \"fit\", \"trop\", \"dès\", \"chambre\", \"maison\", \"hui\", \"dem\", \"louer\", \"etc\", \n",
    "       \"peu\", \"leurs\", \"ans\", \"ceux\", \"rue\", \"ecr\", \"voici\", \"celui\", \"cctto\", \"qu'il\", \"d'un\",\n",
    "       \"jour\", \"francs\", \"bon\", \"quelques\", \"soir\", \"toute\", \"saint\", \"mois\", \"chez\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regroupement de nos fichiers en un nouveau fichier .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/caf/*.txt > ../data/allcaf.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenisation du nouveau fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du contenu du fichier\n",
    "path = \"../data/allcaf.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "print(f\"{len(words)} words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les 10 premiers mots \n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Représentation du vocabulaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimination les stopwords et les termes non alphabétiques\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des mots les plus fréquents pour en faire un plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mots les plus fréquent\n",
    "fdist = nltk.FreqDist(kept)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: les n mots les plus fréquents\n",
    "n = 20\n",
    "fdist.plot(n, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETTOYAGE DU CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrage du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import sys\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Répertoire d'inputs et d'outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le répertoire qui contient vos fichiers txt exportés de Camille\n",
    "indir = \"../data\"\n",
    "# Le répertoire qui contiendra les fichiers txt nettoyés\n",
    "outdir = \"../data/clean\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Termes de recherches dans le corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"artiste\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction de phrases contenant le terme de la recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(indir)[:10]:\n",
    "    if file.endswith(\".txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage du contenu du fichier dans une liste\n",
    "content_list = []\n",
    "path = \"../data/clean/1900.txt\"\n",
    "limit = 10**8\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impression des 200 premiers caractères du contenu du premier fichier\n",
    "content_list[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecriture de tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(path, 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impression du contenu du fichier et constat des \"déchets\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Application de la fonction de nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction sur le fichier complet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "clean_text(year, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du résultat\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSE APPROFONDIE ET ENRICHISSEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de mots clés "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=20)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/clean\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_file = files[0]\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path, this_file), 'r', encoding='utf-8').read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du nuage de mots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnaissance et extraction d'entités nommées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/clean/1900.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entités personnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=False)\n",
    "\n",
    "for person, freq in sorted_people[:100]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entités Lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "location = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        location[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_location = sorted(location.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for location, freq in sorted_location[:50]:\n",
    "    print(f\"{location} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entités organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "organisation = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        organisation[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_organisation = sorted(organisation.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for organisation, freq in sorted_organisation[:50]:\n",
    "    print(f\"{organisation} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fonction get_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse du sentiment de quelques phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Ida Rubinstein jooie le rôle, moina en femme douloureuse qu'en artiste ayant un souci constant de l'esthétique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Podlesnaia, qui incarne la femme fatale est très belle, et comme femme, et comme artiste.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Tout le monde, en France, aime Claudette, parce que c'est une grande, une véritable artiste, doublée d'une femme simple, avenante, gentille.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Régina Bruylant est une jeune artiste qui, après un an et demi /e travail, vient d'ouvrir une exposition dans une modeste salle de la chaussée de Ninove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Madeleine Deltenre n'en reste pas moins une véritable artiste vibrante et sincère dont la personnalité ne tardera pas, nous en sommes persuadés, à s'imposer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Françoise Rosay est une femme intelligente, une artiste de talent et l'excellente maman de trois grands garçons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Mlle dc a Bruchollerie mille fois mieux qu'une virtuose : simplement et grandement une artiste >.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Mme Madeleine Soria prête au personnage de Zaza sa personnalité de parfaite artiste et de femme sensible, jouant son rôle de manière profondément humaine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La Christiani (le nom d'une célèbre artiste, une femme qui joue du violoncelle comme Servais, Demunck ou Batta),\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"On ne nous donnera plus une Hélène belle commo WlDelvil, était une artiste délibérée et une femme jolie, sach mt draper do distinction dos airs de luronne.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Mme de JRudder, — encore une femme artiste, d'un enthousiasme et d'une vaillance incomparables et à laquelle nous consacrerons un iour une étude complète,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation des transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librairies et imports \n",
    "!pip install tensorflow\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\", use_pt=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Bonne, généreuse .charitable, d'une' piété éclairée, d'une grande, intelligence, amijé, des arts, artiste elle-même, c'était une: femme vrajpnent supérieuse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Mademoiselle VERA GORSKA Artiste dramatique La jeune femme justifiait ce litre par les rôles de figurante qu'elle remplissait quelquefois dans les théâtres de troisième ordre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Je suis tout d'abord, et avant tout, une artiste, je ne suis femme que secondairement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"L'aetrice.quelque admirable qu'elle soit comme artiste, reste femme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Elle- est artiste et mutine; elle est femme et cependant poupée ; elle est gracieuse ; elle est belle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Une femme, une faible femme, artiste elle-même, a vaincu l'entêtement du P. Boom.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Contre elle, il y a sa jeunesse : une qualité chez la femme, un défaut pour l'artiste !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
